{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Jantahack_HR_Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-RInExpThAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount the drive to access the data from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFihYuocsPEz",
        "colab_type": "text"
      },
      "source": [
        "1. Basemodel XGboost - No imputation, but scale_pos\n",
        "Valid Score = 0.7567241158438351, LBscore =0.6794885791\n",
        "2. Impute = frequent for missing,Using SMOTE to remove class imbalance, XGBoost with 10 foldCV.\n",
        "Valid Score =0.9678402054766864,LBscore = 0.668696479543292\n",
        "3.Impute = frequent, with class weights on,\n",
        "Valid Score =0.7518523186494807,LB Score=0.6823848280\n",
        "4. Removing city column to avoid higher dimensions, with \n",
        "Valid Score=0.7719847981169748,LBscore=0.67915726879984"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf4tL54PT0R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pyforest\n",
        "!pip install pycaret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dAYGZazLgJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install feature_engine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QH1mmcygsYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install h2o"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvy031xrTmVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import itertools\n",
        "import statsmodels.api as sm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "#import pyforest\n",
        "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor,AdaBoostRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import xgboost\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "#from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import StackingClassifier, AdaBoostClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "\n",
        "# from feature_engine.categorical_encoders import OneHotCategoricalEncoder\n",
        "# from feature_engine.categorical_encoders import CountFrequencyCategoricalEncoder, OrdinalCategoricalEncoder\n",
        "# from feature_engine.categorical_encoders import MeanCategoricalEncoder,WoERatioCategoricalEncoder\n",
        "# from feature_engine.discretisers import DecisionTreeDiscretiser, EqualFrequencyDiscretiser, EqualWidthDiscretiser\n",
        "\n",
        "#import classification module\n",
        "#from pycaret.classification import *\n",
        "\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EaM9O8fTxg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/JantaHack_HR_Analytics/Data/train_jqd04QH.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/JantaHack_HR_Analytics/Data/test_KaymcHn.csv')\n",
        "ss = pd.read_csv('/content/drive/My Drive/JantaHack_HR_Analytics/Data/sample_submission_sxfcbdx.csv')\n",
        "\n",
        "cols_to_drop = ['enrollee_id']\n",
        "train = train.drop(cols_to_drop,axis=1)\n",
        "test = test.drop(cols_to_drop,axis=1)\n",
        "whole_data = pd.concat([train,test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMc2AHutek7v",
        "colab_type": "code",
        "outputId": "be378268-33a3-4dad-d5b1-1d01e5124727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "whole_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33380, 13)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC9F7D3eOZPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CV model to find tuned XGBoost model. \n",
        "def xgb_train_model(X,y):\n",
        "  # seed 1301 is highest, 130 gave more than 1301 depth=7, n_estimators=1000\n",
        "  xgtrain = xgb.DMatrix(X, label=y)\n",
        "  clf = xgb.XGBClassifier(max_depth = 7,\n",
        "                  n_estimators=1000,\n",
        "                  learning_rate=0.01, \n",
        "                  nthread=4,\n",
        "                  subsample=1.0,\n",
        "                  colsample_bytree=0.5,\n",
        "                  min_child_weight = 3,\n",
        "                  seed=130)\n",
        "  xgb_param = clf.get_xgb_params()\n",
        "  #do cross validation\n",
        "  print ('Start cross validation')\n",
        "  cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=10, metrics=['auc'],\n",
        "      early_stopping_rounds=50, stratified=True, seed=1301)\n",
        "  print('Best number of trees = {}'.format(cvresult.shape[0]))\n",
        "  clf.set_params(n_estimators=cvresult.shape[0])\n",
        "\n",
        "  print('Fit on the trainingsdata')\n",
        "  clf.fit(X, y, eval_metric='auc')\n",
        "  print('Overall AUC:', roc_auc_score(y, clf.predict_proba(X)[:,1]))\n",
        "  return clf,cvresult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW95T2AEMgtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "whole_data_pre = whole_data.copy()\n",
        "\n",
        "# Manually selecting ordinal and nominal categorical columns to do different encodings\n",
        "nom_cat = ['city','gender','relevent_experience','major_discipline','company_type']\n",
        "whole_data_pre = pd.get_dummies(whole_data, columns=nom_cat, drop_first=True)\n",
        "\n",
        "ord_cat = ['education_level','company_size','last_new_job','enrolled_university','experience']\n",
        "\n",
        "ed_lev = train['education_level'].unique().tolist()[:4] + [train['education_level'].unique().tolist()[5]]\n",
        "#ed_lev_values = [21,23,17,28,15] # based on age at which the graduation finished(this improved score a bit)\n",
        "ed_lev_values = [3,4,2,5,1]\n",
        "ed_lev_map = dict(zip(ed_lev, ed_lev_values))\n",
        "whole_data_pre['education_level'] = whole_data_pre['education_level'].replace(ed_lev_map)\n",
        "\n",
        "comp_size = train['company_size'].unique().tolist()[:3] + train['company_size'].unique().tolist()[4:]\n",
        "comp_size_values = [4,1,3,7,8,6,5,2]\n",
        "comp_size_map = dict(zip(comp_size, comp_size_values))\n",
        "whole_data_pre['company_size'] = whole_data_pre['company_size'].replace(comp_size_map)\n",
        "\n",
        "last_newj = train['last_new_job'].unique().tolist()[:6]\n",
        "last_newj_values = [1,2,3,5,0,4]\n",
        "last_newj_map = dict(zip(last_newj, last_newj_values))\n",
        "whole_data_pre['last_new_job'] = whole_data_pre['last_new_job'].replace(last_newj_map)\n",
        "\n",
        "enr_uni = train['enrolled_university'].unique().tolist()[:3]\n",
        "enr_uni_values = [0,2,1]\n",
        "enr_uni_map = dict(zip(enr_uni, enr_uni_values))\n",
        "whole_data_pre['enrolled_university'] = whole_data_pre['enrolled_university'].replace(enr_uni_map)\n",
        "\n",
        "exper = train['experience'].unique().tolist()[:21]+[train['experience'].unique().tolist()[22]]\n",
        "exper_values = [3,14,6,8,21,4,9,15,10,1,5,16,11,12,7,2,13,0,19,18,17,20]\n",
        "exper_map = dict(zip(exper,exper_values))\n",
        "whole_data_pre['experience'] = whole_data_pre['experience'].replace(exper_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzPP-NysM_LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pre = whole_data_pre.iloc[:len(train)]\n",
        "test_pre = whole_data_pre.iloc[len(train):]\n",
        "test_pre.drop(['target'],axis=1,inplace=True)\n",
        "\n",
        "train_pre['no_of_missing'] = train_pre.isnull().sum(axis=1)\n",
        "train_pre['is_missing'] = np.where(train_pre.isnull().sum(axis=1) == 0, 0,1)\n",
        "\n",
        "test_pre['no_of_missing'] = test_pre.isnull().sum(axis=1)\n",
        "test_pre['is_missing'] = np.where(test_pre.isnull().sum(axis=1) == 0, 0,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMSN-H1PLWPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = train_pre.drop('target',axis=1)\n",
        "y = train_pre['target']\n",
        "\n",
        "# define the imputer\n",
        "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
        "X = imputer.fit_transform(X)\n",
        "X = pd.DataFrame(X,columns=train_pre.drop('target',axis=1).columns)\n",
        "\n",
        "test_pre_imp = imputer.transform(test_pre)\n",
        "test_pre_imp = pd.DataFrame(test_pre_imp,columns= test_pre.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CASABamniRQx",
        "colab_type": "code",
        "outputId": "2369210d-1d6e-4e76-de3b-2f250bca0367",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "clf,cvresult = xgb_train_model(X,y)\n",
        "pred = clf.predict_proba(test_pre,ntree_limit=cvresult.shape[0])\n",
        "ID = pd.read_csv('/content/drive/My Drive/JantaHack_HR_Analytics/Data/test_KaymcHn.csv').enrollee_id\n",
        "submission = pd.DataFrame({\"enrollee_id\":ID, \"target\":pred[:,1]})\n",
        "name_of_submission = 'Final_Submission.csv'\n",
        "submission.to_csv(name_of_submission,index=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start cross validation\n",
            "Best number of trees = 200\n",
            "Fit on the trainingsdata\n",
            "Overall AUC: 0.7359366277647874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RRxdRnwWE2O",
        "colab_type": "text"
      },
      "source": [
        "Important Points\n",
        "1. Tried lot of categorical encodings and combination of those, but score hasn't been improved from the above model\n",
        "\n",
        "2.  Different approaches can be using different models like Lightgbm and Catboots, then use stacking on top those to give better score.\n",
        "\n",
        "3. So concluded that best model has simple imputation(mode) with one hot encoding to nominal categories and label encoding(manually done) for ordinal encodings. Modelling using Xgboost CV.\n",
        "\n",
        "Note: Class is imbalanced, so choosing cross-validation method is key in this competition\n",
        "\n",
        "#Thank you for reading this approach"
      ]
    }
  ]
}